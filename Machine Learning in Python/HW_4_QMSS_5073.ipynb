{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 4 - QMSS 5073.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXoIoruO55Mw"
      },
      "source": [
        "#HW 4 - QMSS 5073 Machine Learning for Social Science\n",
        "##Cindy Chen, cjc2279"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "LW2laAUtNhky"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-01cjsJWAuZ",
        "outputId": "ff555f7c-f079-4a9b-8e69-96c37e3c784c"
      },
      "source": [
        "! pip install -q aimodelshare"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 121 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 13.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 40.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 21.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 300 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 45.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 435 kB 53.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 42.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.0 MB/s \n",
            "\u001b[?25h  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX27iNrT5-eY"
      },
      "source": [
        "#import my common libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMbXEW4NicHV"
      },
      "source": [
        "##Part 1: Build a classification model using text data\n",
        "\n",
        "ANSWER:\n",
        "> I built three classification models and ran a LogisticRegression on the data.  I used CountVectorizer and TfidfVectorizer, and tried different n-gram and document feature parameters.  Out of my three trials, my second model using the following parameters, vectorizers, and other considerations:\n",
        "\n",
        "* TfidfVectorizer\n",
        "* the inclusion of stop words\n",
        "* A C parameter of 100 (in other words, more freedom in the decision boundary)\n",
        "* inclusion of single words and bi-grams\n",
        "\n",
        "> yielded the highest f1 score of 0.96. \n",
        "\n",
        "> It is important to note that f-1 performance was critical to my evaluation of my model since it considers false positives and false negatives in determining accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ7fVIrz8Ffl"
      },
      "source": [
        "Load aimodelshare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LG8LfcD52AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1beeabff-977a-42cc-8886-0bd8b27191d0"
      },
      "source": [
        "import aimodelshare as ai\n",
        "X_train, X_test, y_train_labels, y_test_labels, example_data, lstm_model, lstm_model2 = ai.import_quickstart_data(\"clickbait\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [==============================================>  ]\n",
            "\n",
            "Data downloaded successfully.\n",
            "\n",
            "Preparing downloaded files for use...\n",
            "\n",
            "Success! Your Quick Start materials have been downloaded. \n",
            "You are now ready to run the tutorial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAkAXXgl8H3V"
      },
      "source": [
        "Step 1. Preprocess my data by using **CountVectorizer** to vectorize the clickbait text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "352peThQ6RDb"
      },
      "source": [
        "vect = CountVectorizer(stop_words = \"english\").fit(X_train)\n",
        "X_train_2 = vect.transform(X_train)\n",
        "\n",
        "#fit X_test using transform\n",
        "X_test_2 = vect.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gah53CvycNoj"
      },
      "source": [
        "Recode my y-labels into binary 1 and 0 values. This is necessary if I want to use sklearn to evaluate my performance on an f1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7eEKGzTcIde"
      },
      "source": [
        "current_vals_1 = [(y_train_labels == 'clickbait'), (y_train_labels == 'not clickbait')]\n",
        "current_vals_2 = [(y_test_labels == 'clickbait'), (y_test_labels == 'not clickbait')]\n",
        "recoded_vals = [1, 0]\n",
        "\n",
        "y_train_labels = np.select(current_vals_1, recoded_vals, default = np.nan)\n",
        "y_test_labels = np.select(current_vals_2, recoded_vals, default = np.nan)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkSDdadQb_1h"
      },
      "source": [
        "Review my data before I run any logistic regressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhvUlBG-cAym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3243171-dcd1-499f-bfca-4741109b2747"
      },
      "source": [
        "feature_names = vect.get_feature_names()\n",
        "print(\"Number of features: {}\".format(len(feature_names)))\n",
        "print(\"Every 500th feature:\\n{}\".format(feature_names[::500]))\n",
        "print(\"X data shape: \", X_train.shape, X_test.shape)\n",
        "print(\"Y data shape: \", y_train_labels.shape, y_test_labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 20066\n",
            "Every 500th feature:\n",
            "['00', 'accident', 'anchor', 'austrian', 'belonging', 'bradley', 'canes', 'choppy', 'conducting', 'crowne', 'denomination', 'donald', 'emerges', 'fabiola', 'flush', 'genuine', 'guns', 'hipster', 'imprisons', 'jackets', 'kmart', 'limbo', 'marble', 'militants', 'nacional', 'ode', 'parks', 'plaintiffs', 'professors', 'ravaged', 'resisting', 'rut', 'separate', 'slater', 'stacey', 'superstitious', 'texting', 'tripathi', 'using', 'website', 'zealand']\n",
            "X data shape:  (24979,) (6245,)\n",
            "Y data shape:  (24979,) (6245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KywHfwkuJOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce8789a-ce0d-4e11-b102-de73c47e5900"
      },
      "source": [
        "scores = cross_val_score(LogisticRegression(), X_train_2, y_train_labels, cv = 5, scoring = \"f1\")\n",
        "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean cross-validation accuracy: 0.949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ov8wo5hYlj1"
      },
      "source": [
        "Run GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66TSkrBNKGjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1bb897-21cf-48f5-9708-bd8a5fbc057e"
      },
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "gridsearch = GridSearchCV(LogisticRegression(max_iter = 10000), param_grid = param_grid, cv = 5, scoring= \"f1\")\n",
        "gridsearch.fit(X_train_2, y_train_labels)\n",
        "y_pred = gridsearch.predict(X_test_2)\n",
        "\n",
        "print(\"Test score: {:.2f}\".format(gridsearch.score(X_test_2, y_test_labels)))\n",
        "print(\"Best cross-validation score: {:.2f}\".format(gridsearch.best_score_))\n",
        "print(\"Best parameters: \", gridsearch.best_params_)\n",
        "print(classification_report(y_test_labels, y_pred))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.95\n",
            "Best cross-validation score: 0.95\n",
            "Best parameters:  {'C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.95      0.96      3223\n",
            "         1.0       0.95      0.95      0.95      3022\n",
            "\n",
            "    accuracy                           0.95      6245\n",
            "   macro avg       0.95      0.95      0.95      6245\n",
            "weighted avg       0.95      0.95      0.95      6245\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaWlIfcYfoA"
      },
      "source": [
        "Step 2. Preprocess my data by using **TfidfVectorizer** (using an ngram_range of 1 and 2) to vectorize the clickbait text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNMT-MgzdSCz"
      },
      "source": [
        "tfvect = TfidfVectorizer(ngram_range=(1,2), stop_words = \"english\").fit(X_train)\n",
        "X_train_tf2 = tfvect.transform(X_train)\n",
        "\n",
        "#fit X_test using transform\n",
        "X_test_tf2 = tfvect.transform(X_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGRThUtDY6gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67e8839-2a95-429b-b15f-6896064b3d83"
      },
      "source": [
        "feature_names = tfvect.get_feature_names()\n",
        "print(\"Number of features: {}\".format(len(feature_names)))\n",
        "print(\"Every 500th feature:\\n{}\".format(feature_names[::500]))\n",
        "print(\"X data shape: \", X_train_tf2.shape, X_test_tf2.shape)\n",
        "print(\"Y data shape: \", y_train_labels.shape, y_test_labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 117416\n",
            "Every 500th feature:\n",
            "['00', '12 disney', '15 nearly', '18 adorable', '20 life', '21 candid', '24 mind', '290', '43 marathons', '89 dead', 'access saudi', 'actress brittany', 'administration changes', 'afghan troops', 'ahs coven', 'albus dumbledore', 'america biggest', 'anil', 'ants protect', 'architect dies', 'art decor', 'astronauts let', 'australia lose', 'away ugandan', 'bake booze', 'bargaining power', 'bbc play', 'begins wobbly', 'best sweater', 'billion bailout', 'blasting mtv', 'bogosian writing', 'born terrorist', 'brazilian soccer', 'british fast', 'buffay supposed', 'busts', 'called piecaken', 'canadians won', 'cardinals', 'castroneves wins', 'celebrity ex', 'change omani', 'cheaper', 'chilean earthquake', 'christmas decorations', 'claims men', 'close ranks', 'colleagues gives', 'commander face', 'concerned situation', 'considers loyalty', 'cooker wrong', 'councilors', 'crash brazil', 'crisis short', 'cup rugby', 'daily sunday', 'dawson actually', 'deal surge', 'deepwater', 'delivers apology', 'designer best', 'dicaprio bradley', 'dinner recipe', 'disney animator', 'documents alleging', 'dora', 'drinkers', 'dunlap coaching', 'eating disorder', 'election ends', 'empathy takes', 'entertainment divisions', 'eu reports', 'executed', 'extends state', 'failed attack', 'fanatic', 'favourite fall', 'festival held', 'final world', 'flag richmond', 'focus scrutiny', 'forecasts expects', 'fraud suspected', 'frustrating final', 'gallic', 'gebrselassie', 'ghanaian', 'gisela', 'gold world', 'government accountability', 'greece athens', 'guantanamo prosecutors', 'guys dated', 'halts heavy', 'hardest premier', 'headset', 'help people', 'hilarious revelations', 'holdouts cast', 'hopping', 'hover', 'ibori arrested', 'image david', 'incredible things', 'informal', 'instagram post', 'interviews stephen', 'iraq duty', 'issue standards', 'japanese player', 'jockey stathi', 'juice', 'kardashian thanksgiving', 'kidnaps daughter', 'kills fourteen', 'know emojis', 'kurmanbek bakiev', 'large', 'lawsuits torrent', 'leave year', 'level maximum', 'like decade', 'lirr', 'lng plant', 'looks sell', 'loves american', 'maffra games', 'make tube', 'man transform', 'marriage vermont', 'mayhew', 'meeting falling', 'mess crocodile', 'milan lučić', 'minister trudeau', 'mochrie host', 'months making', 'movie sisters', 'mushroom corals', 'nasser', 'needed iran', 'new holiday', 'nicole', 'noticed arthur', 'obama volcker', 'officially model', 'ontario place', 'original bands', 'owner tom', 'panel oks', 'party run', 'peak form', 'people smiling', 'persons', 'photos pure', 'pioneer builds', 'plant experiences', 'pointlessness', 'pop tart', 'potter comic', 'presented', 'princess reaction', 'profit washington', 'proud woman', 'puppies casts', 'questions doctor', 'railway', 'raw photos', 'really hard', 'recipes celebrate', 'refused permission', 'relief home', 'report criticizes', 'resolve', 'reveal brief', 'ridley cool', 'robot cast', 'rotblat', 'runways destroyed', 'sales continue', 'saw single', 'school flood', 'scully', 'sedition', 'senior knows', 'sets sweeping', 'sharpen focus', 'short girl', 'signing day', 'sitting pretty', 'small cars', 'soil samples', 'sours dubai', 'spencer', 'st paul', 'starting point', 'stewards', 'stranded mars', 'student makes', 'suffer fomo', 'support war', 'sweating relate', 'taken baseball', 'targaryen look', 'teen post', 'territorial', 'theories totally', 'thirteen dead', 'tigers confirm', 'tinder need', 'topples chief', 'trade imbalances', 'treats need', 'trubridge breaks', 'turkeys wondering', 'twins minnesota', 'umeå', 'unity upsets', 'usc defeats', 've danced', 'video hotline', 'voice feels', 'wands characters', 'waste', 'wayyy', 'wellpoint sells', 'wikipedia internet', 'wins opener', 'women painting', 'world championships', 'xbox division', 'younger']\n",
            "X data shape:  (24979, 117416) (6245, 117416)\n",
            "Y data shape:  (24979,) (6245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ovRYShZIma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f50672-052b-4e5d-cc38-f1a9d2be9ed2"
      },
      "source": [
        "scores = cross_val_score(LogisticRegression(), X_train_tf2, y_train_labels, cv = 5, scoring = \"f1\")\n",
        "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean cross-validation accuracy: 0.937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ4BzQ9nZP-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7659d2-ef33-49a3-8c6b-b44007493174"
      },
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "gridsearch = GridSearchCV(LogisticRegression(max_iter = 10000), param_grid = param_grid, cv = 5, scoring= \"f1\")\n",
        "gridsearch.fit(X_train_tf2, y_train_labels)\n",
        "y_pred_tf = gridsearch.predict(X_test_tf2)\n",
        "\n",
        "print(\"Test score: {:.2f}\".format(gridsearch.score(X_test_tf2, y_test_labels)))\n",
        "print(\"Best cross-validation score: {:.2f}\".format(gridsearch.best_score_))\n",
        "print(\"Best parameters: \", gridsearch.best_params_)\n",
        "print(classification_report(y_test_labels, y_pred_tf))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.96\n",
            "Best cross-validation score: 0.95\n",
            "Best parameters:  {'C': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.96      0.96      3223\n",
            "         1.0       0.95      0.96      0.96      3022\n",
            "\n",
            "    accuracy                           0.96      6245\n",
            "   macro avg       0.96      0.96      0.96      6245\n",
            "weighted avg       0.96      0.96      0.96      6245\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCZsC2Kgljy"
      },
      "source": [
        "Step 1. Preprocess my data by using **TfidfVectorizer** to vectorize the clickbait text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3OzmCMgglxH"
      },
      "source": [
        "tf2vect = TfidfVectorizer(ngram_range=(1,3), min_df = 5, stop_words = \"english\").fit(X_train)\n",
        "X_train_tf3 = tf2vect.transform(X_train)\n",
        "\n",
        "#fit X_test using transform\n",
        "X_test_tf3 = tf2vect.transform(X_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELvPLSP-kcJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe208b3f-5a0e-4a87-e156-3c30e4e2f166"
      },
      "source": [
        "feature_names = tf2vect.get_feature_names()\n",
        "print(\"Number of features: {}\".format(len(feature_names)))\n",
        "print(\"Every 500th feature:\\n{}\".format(feature_names[::500]))\n",
        "print(\"X data shape: \", X_train_tf3.shape, X_test_tf3.shape)\n",
        "print(\"Y data shape: \", y_train_labels.shape, y_test_labels.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 6932\n",
            "Every 500th feature:\n",
            "['000', 'alonso', 'boycott', 'complaints', 'don want', 'flying', 'holiday', 'kosovo', 'mental', 'panda', 'raccoon', 'seek', 'sunny philadelphia', 'unexpected']\n",
            "X data shape:  (24979, 6932) (6245, 6932)\n",
            "Y data shape:  (24979,) (6245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na5okI5Ukiry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f570f08a-c53c-40ec-aaf1-0b1576956b54"
      },
      "source": [
        "scores = cross_val_score(LogisticRegression(), X_train_tf3, y_train_labels, cv = 5, scoring = \"f1\")\n",
        "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean cross-validation accuracy: 0.948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPukT84k7GI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae25919-a8b7-48c9-8d15-cdf5cf98d3bd"
      },
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "gridsearch = GridSearchCV(LogisticRegression(max_iter = 10000), param_grid = param_grid, cv = 5, scoring= \"f1\")\n",
        "gridsearch.fit(X_train_tf3, y_train_labels)\n",
        "y_pred_tf = gridsearch.predict(X_test_tf3)\n",
        "\n",
        "print(\"Test score: {:.2f}\".format(gridsearch.score(X_test_tf3, y_test_labels)))\n",
        "print(\"Best cross-validation score: {:.2f}\".format(gridsearch.best_score_))\n",
        "print(\"Best parameters: \", gridsearch.best_params_)\n",
        "print(classification_report(y_test_labels, y_pred_tf))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.95\n",
            "Best cross-validation score: 0.95\n",
            "Best parameters:  {'C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.96      0.95      3223\n",
            "         1.0       0.96      0.95      0.95      3022\n",
            "\n",
            "    accuracy                           0.95      6245\n",
            "   macro avg       0.95      0.95      0.95      6245\n",
            "weighted avg       0.95      0.95      0.95      6245\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3xAhcEIi6t5"
      },
      "source": [
        "##Part 2: Build a predictive neural network using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G9jpu2liz93"
      },
      "source": [
        "####Include code for selecting the number of hidden units using GridSearchCV and evaluation on a test-set.  Describe the differences in the predictive accuracy of models with different numbers of hidden units.  Describe the predictive strength of your best model.  Be sure to explain your choice and evaluate this model using the test set.\n",
        "\n",
        "**ANSWER:**\n",
        "> In using GridSearch for 1, 3, 5, 10, and 20 hidden nodes, my best model employed 5 nodes in the first layer.  In using mean test scores (which are based on my training data) in my GridSearch to evaluate the predictive accuracy of my models with different numbers of hidden units, my model with 20 nodes had a mean predictive accuracy of 0.851.  The next three models in terms of accuracy were quite similar in scores: my model with 10 hidden nodes had a mean predictive accuracy of 0.721, followed by my mdoel with 3 hidden nodes with a mean predictive accuracy of 0.707, and then my model with 5 hidden nodes (predictive accuracy of 0.696).  Finally, my worst performing model was my model with one hidden unit, where the mean predictive accuracy was 0.465.\n",
        "\n",
        "> In running GridSearchCV, my best model involved **20** hidden units and yielded a **training predictive accuracy of 0.851**.  This was the model that I chose as my final model. When I evaluated this same model on my test set, I encountered a test score accuracy of **0.924**.  I chose the number of epochs to be 25, because I did not want to overfit, and this strategy proved to be effective since my predictive accuracy was higher on my test set than my training data.  I also used a validation set (using 20% of my data) to help me tune my models.\n",
        "\n",
        "> As an aside, I noticed that scaling my data dramatically improved the performance of my neural network prediction accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCsKJPl1nf-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63af91ad-5224-4d91-eebe-c9cd3f68f323"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "iris_data = pd.read_csv(\"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\")\n",
        "iris_data.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide my data into training, testing, and validation sets.  Scale my X data as well."
      ],
      "metadata": {
        "id": "k6WlxrpCUsli"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvm2EKO1s1Ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce18a166-7d7e-4c9d-dae6-8c667bcdb7f5"
      },
      "source": [
        "#set up my data so I can create train and test sets\n",
        "iris_X = iris_data.drop(\"Species\", axis = 1)\n",
        "iris_y = iris_data['Species']\n",
        "\n",
        "#turn my labels into dummy variables\n",
        "iris_y_converted = pd.get_dummies(iris_y)\n",
        "#iris_y_converted = to_categorical(iris_y, num_classes = 3)\n",
        "\n",
        "#train test split process\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y_converted)\n",
        "\n",
        "#scale my X data\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "X_train_scaled = sc.transform(X_train)\n",
        "\n",
        "print(\"X train shape: \", X_train_scaled.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train shape:  (112, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create 2 hidden layers."
      ],
      "metadata": {
        "id": "sSNH9ivxUz9x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ldybmhli-Mz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a97cb1-a7a7-45d9-a4aa-d762bed24129"
      },
      "source": [
        "#create two hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(3, activation='relu', input_dim=5))\n",
        "model.add(Dense(3, activation='softmax', input_dim=5))\n",
        "optimizer = SGD(learning_rate = 0.1)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='AUC')\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=10, batch_size=128, validation_split=.2)\n",
        "\n",
        "score = model.evaluate(X_test_scaled, y_test, batch_size=128) # extract loss and accuracy from test data evaluation\n",
        "print(score)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8406 - auc: 0.7335 - val_loss: 0.7882 - val_auc: 0.8048\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.8243 - auc: 0.7765 - val_loss: 0.7742 - val_auc: 0.8171\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8097 - auc: 0.8030 - val_loss: 0.7612 - val_auc: 0.8393\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7960 - auc: 0.8315 - val_loss: 0.7488 - val_auc: 0.8559\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7829 - auc: 0.8560 - val_loss: 0.7372 - val_auc: 0.8715\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7707 - auc: 0.8722 - val_loss: 0.7263 - val_auc: 0.8875\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7595 - auc: 0.8826 - val_loss: 0.7174 - val_auc: 0.8932\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7493 - auc: 0.8871 - val_loss: 0.7089 - val_auc: 0.9003\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7397 - auc: 0.8926 - val_loss: 0.7011 - val_auc: 0.9074\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7308 - auc: 0.9017 - val_loss: 0.6935 - val_auc: 0.9121\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7811 - auc: 0.8674\n",
            "[0.7811253666877747, 0.867382287979126]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#just so I know what my first model looks like\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "84gqJvH5FCjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c240d658-be88-4625-82a1-160068e11eaf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " module_wrapper (ModuleWrapp  (None, 3)                18        \n",
            " er)                                                             \n",
            "                                                                 \n",
            " module_wrapper_1 (ModuleWra  (None, 3)                12        \n",
            " pper)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try GridSearchCV to find the best number of hidden nodes."
      ],
      "metadata": {
        "id": "fKdNa70BU27s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL2q_-s03Ls2"
      },
      "source": [
        "#write this function\n",
        "def create_model(hiddennodes=1):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(hiddennodes, activation=\"relu\")) #layer 1\n",
        "\tmodel.add(Dense(3, activation=\"relu\")) #layer 2; my second hidden layer must have 3 units since I have 3 categories\n",
        "\t# Compile model\n",
        "\toptimizer = SGD(learning_rate = 0.1)\n",
        "\tmodel.compile(loss='categorical_crossentropy',\n",
        "\t              optimizer=optimizer,\n",
        "\t\t\t\t\t\t\t\t       metrics='accuracy') #i ran it on accuracy instead of AUC because it didn't yield any results\n",
        "\treturn model\n",
        "\n",
        "model2 = KerasClassifier(build_fn = create_model, epochs = 25, verbose = 0)\n",
        "\n",
        "#create my parameter grid\n",
        "param_grid = dict(hiddennodes=[1,3,5,10,20])\n",
        "\n",
        "#run GridSearch\n",
        "grid = GridSearchCV(estimator=model2, param_grid=param_grid)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review the results of my gridsearch\n",
        "pd.DataFrame(grid.cv_results_)"
      ],
      "metadata": {
        "id": "BPZT_IzgFfhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "3a2943d9-0191-4986-9212-c3aacb24f98d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_hiddennodes</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.767175</td>\n",
              "      <td>0.010710</td>\n",
              "      <td>0.169256</td>\n",
              "      <td>0.006379</td>\n",
              "      <td>1</td>\n",
              "      <td>{'hiddennodes': 1}</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.464822</td>\n",
              "      <td>0.190690</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.756961</td>\n",
              "      <td>0.013510</td>\n",
              "      <td>0.166421</td>\n",
              "      <td>0.009112</td>\n",
              "      <td>3</td>\n",
              "      <td>{'hiddennodes': 3}</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.707115</td>\n",
              "      <td>0.148862</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.828539</td>\n",
              "      <td>0.184270</td>\n",
              "      <td>0.176117</td>\n",
              "      <td>0.011587</td>\n",
              "      <td>5</td>\n",
              "      <td>{'hiddennodes': 5}</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.954545</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.696443</td>\n",
              "      <td>0.209697</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.731749</td>\n",
              "      <td>0.028377</td>\n",
              "      <td>0.177881</td>\n",
              "      <td>0.011645</td>\n",
              "      <td>10</td>\n",
              "      <td>{'hiddennodes': 10}</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.721344</td>\n",
              "      <td>0.236045</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.744904</td>\n",
              "      <td>0.027083</td>\n",
              "      <td>0.164974</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>20</td>\n",
              "      <td>{'hiddennodes': 20}</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850593</td>\n",
              "      <td>0.159684</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0       0.767175      0.010710  ...        0.190690                5\n",
              "1       0.756961      0.013510  ...        0.148862                3\n",
              "2       0.828539      0.184270  ...        0.209697                4\n",
              "3       0.731749      0.028377  ...        0.236045                2\n",
              "4       0.744904      0.027083  ...        0.159684                1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate my predictions to determine the f1 score of my best model through GridSearchCV."
      ],
      "metadata": {
        "id": "G2MHbzaFVLlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds_classindex = grid_result.predict(X_test_scaled)\n",
        "\n",
        "#transform my y_test into labels\n",
        "y_test_converted = y_test.idxmax(axis=1)\n",
        "y_test_converted = y_test_converted.replace(\"setosa\", 0)\n",
        "y_test_converted = y_test_converted.replace(\"versicolor\", 1)\n",
        "y_test_converted = y_test_converted.replace(\"virginica\", 2)"
      ],
      "metadata": {
        "id": "37oaeE2GYkuf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best performance on training data: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "#evaluate predictions on average f1 score\n",
        "f1_score_ypred = f1_score(y_test_converted, ypreds_classindex, average=\"macro\")\n",
        "print(\"Result on test set: {:.3f}\".format(f1_score_ypred))"
      ],
      "metadata": {
        "id": "Krn0x3INUhrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587a1032-ec20-4e8e-d7f2-959ce0045ccb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best performance on training data: 0.850593 using {'hiddennodes': 20}\n",
            "Result on test set: 0.924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/HW_4_QMSS_5073.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh8JZY_GtPBM",
        "outputId": "1605db6e-c187-4aaf-83ce-75c70fab93b1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/HW_4_QMSS_5073.ipynb to html\n",
            "[NbConvertApp] Writing 344188 bytes to /content/HW_4_QMSS_5073.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}