---
title: "QMSS 5058 Final Project: Home Mortgage Disclosure Act Outcomes"
author: "Cindy Chen"
date: "11/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
library(dplyr)
library(tidymodels)
library(tune)
library(additive)
library(themis)
library(rsample)
library(reticulate)
library(gmodels)
reticulate::use_condaenv(condaenv = NULL, conda = "auto", required = FALSE)

set.seed(123)
doParallel::registerDoParallel()
```

## Overview of Data Set

The data set of interest is the Home Mortgage Disclosure Actâ€™s Dynamic National Loan-Level Data Set for the calendar year of 2020. This data is the entire population of mortgage applications submitted in 2020 in the United States, a required disclosure by lenders as part of the Dodd Frank Act.  This is a particularly salient social science data set because we can investigate potential discrimination or issues with fairness related to protected traits like race and gender and the mortgage approval decision-making process.  The data set can be downloaded here: https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level-dataset/2020

The variable "action_taken" is the outcome variable where it originally takes on 8 categorical values.  For the purposes of this study, it will be re-coded as a binary variable. Values that don't denote a final mortgage decision (such as withdrawn applications, incomplete applications, or pre-approval outcomes) will be filtered out of the data set.  Since the purpose of my supervised learning is to predict loan approvals/denials for home purchases, I will filter out other types of loans such as home improvement or refinancing.  Other types of variables within this data set include the applicant age group, their income, loan amount, loan term in months, loan type, preapproval info, debt-to-income ratio, and certain information related to the area that the property is situated are included in the HMDA data set.

## Preprocessing Methodology:

For efficient computing purposes, I decided to import only 50,000 of the ~2 million rows in the original data set.  Since there are 99 total variables where certain variables include a significant portion of values are NAs, I removed any variables where more than 25% of the values were NA. I believe this is the appropriate decision for this data set, because not all variables were expected to be filled out by the applicant. For instance, there are 5 distinct applicant-reported race variables in this data set in case people want to select more than one race.  Of course, most people would only select 1 race and leave the rest blank, so four of the race variables would not contain many values. I also removed derived characteristics from this data set (such as derived race or derived sex), because these variables were derived from pre-existing variables in the data, so they do not contain any new information.

I also filtered out certain values that didn't make sense. Among traits such as sex, age, and race, I removed any non-NA values that denoted that no data was provided or that it was not applicable in the application.

This allowed me to remove 41 variables, bringing my remaining predictors to 58.  These initial filters also reduced my data set down to 8,627 observations.

```{r load_data}
setwd("~/CINDY CHEN/2021 Prep/SCHOOL WORK/Research Project/2020_lar")
mortgage <- read.csv("2020_lar.txt", sep = "|", nrow = 50000)

#list of all the columns in this data table
ncol(mortgage)
```

```{r preprocessing}
#remove any columns with lots of NAs
#i set the threshold to any column with more than 25% of values comprised on NAs
columns_to_remove <- colSums(is.na(mortgage)) %>%
  as.data.frame() %>%
  arrange(desc(.)) %>%
  mutate(NA_prop = ./50000) %>%
  filter(NA_prop > 0.25) %>%
  rownames() %>%
  as.list()

for (i in 1:length(columns_to_remove)) {
mortgage <- mortgage %>%
  select(-columns_to_remove[[i]])}

mortgage <- mortgage %>%
    #remove the variables labelled "derived" because these were not
#collected as part of the original data set and were incorporated after
  select(-starts_with("derived")) %>%
  #Remove any values that don't make sense, like having applicant_sex set to "not applicable"
#or "information not provided"
  filter(action_taken < 4, loan_purpose == 1) %>%
  filter(applicant_sex < 3,co_applicant_sex < 3,
         applicant_race_1 != 6, applicant_race_1 != 7,
         applicant_age != "8888")

#see the variables in the remaining data set
names(mortgage)
```

For my remaining variables, I converted many of my variables to factors, and certain variables were coerced into numeric data, as stipulated by the HDMA documentation of its data fields.  The code sheet for this data set may be found here: https://s3.amazonaws.com/cfpb-hmda-public/prod/help/2018-public-LAR-code-sheet.pdf

For the purposes of my analysis, I decided to remove all co-applicant variables (such as their race, sex, age, etc.), replacing this information with a dummy variable marking whether a co-applicant exists.  I made this decision given the inconsistent data values where a co-applicant doesn't exist on an application, and thought it would not make sense to impute this information if it doesn't exist.  This brings my remaining variables to 51.

```{r clean_data, message = FALSE, cache = TRUE}
#refresh my RAM now that I've reduced the data set
gc()

#set certain variables as factors and others to numeric
mortgage_revised <- mortgage %>%
  mutate(state_code = as.factor(state_code),
         county_code = as.factor(county_code),
         conforming_loan_limit = as.factor(conforming_loan_limit),
         action_taken = as.factor(action_taken),
         purchaser_type = as.factor(purchaser_type),
         preapproval = as.factor(preapproval),
         loan_type = as.factor(loan_type),
         loan_purpose = as.factor(loan_purpose),
         lien_status = as.factor(lien_status),
         reverse_mortgage = as.factor(reverse_mortgage),
         open_end_line_of_credit = as.factor(open_end_line_of_credit),
         business_or_commercial_purpose = as.factor(business_or_commercial_purpose),
         hoepa_status = as.factor(hoepa_status),
         loan_term = as.factor(loan_term),
         negative_amortization = as.factor(negative_amortization),
         balloon_payment = as.factor(balloon_payment),
         interest_only_payment = as.factor(interest_only_payment),
         other_nonamortizing_features = as.factor(other_nonamortizing_features),
         construction_method = as.factor(construction_method),
         occupancy_type = as.factor(occupancy_type),
         manufactured_home_secured_property_type = as.factor(manufactured_home_secured_property_type),
         manufactured_home_land_property_interest = as.factor(manufactured_home_land_property_interest),
         total_units = as.factor(total_units),
         co_applicant_credit_score_type = as.factor(co_applicant_credit_score_type),
         applicant_credit_score_type = as.factor(applicant_credit_score_type),
         applicant_ethnicity_1 = as.factor(applicant_ethnicity_1),
         applicant_ethnicity_observed = as.factor(applicant_ethnicity_observed),
         applicant_race_observed = as.factor(applicant_race_observed),
         applicant_race_1 = as.factor(applicant_race_1),
         co_applicant_ethnicity_1 = as.factor(co_applicant_ethnicity_1),
         co_applicant_ethnicity_observed = as.factor(co_applicant_ethnicity_observed),
         co_applicant_race_1 = as.factor(co_applicant_race_1),
         co_applicant_race_observed = as.factor(co_applicant_race_observed),
         applicant_sex = as.factor(applicant_sex),
         co_applicant_sex = as.factor(co_applicant_sex),
         applicant_sex_observed = as.factor(applicant_sex_observed),
         co_applicant_sex_observed = as.factor(co_applicant_sex_observed),
         applicant_age = as.factor(applicant_age),
         co_applicant_age = as.factor(co_applicant_age),
         applicant_age_above_62 = as.factor(applicant_age_above_62),
         submission_of_application = as.factor(submission_of_application),
         initially_payable_to_institution = as.factor(initially_payable_to_institution),
         aus_1 = as.factor(aus_1),
         denial_reason_1 = as.factor(denial_reason_1),
         property_value = as.numeric(property_value)) %>%
  #recode some of the factors
  mutate(action_taken = recode_factor(action_taken, '1' = "approved", 
                                      '2' = "approved",
                                      '3' = "denied"),
         preapproval = recode_factor(preapproval, '1' = "preapproval requested", 
                                     '2' = "preapproval not requested",
                                     .default = NA_character_),
         loan_type = recode_factor(loan_type, '1' = "conventional",
                                   '2' = "FHA", 
                                   '3' = "VA", 
                                   '4' = "USDA Rural"),
         lien_status = recode_factor(lien_status, '1' = 'secured_first_lien',
                                     '2' = "secured_subordinate_lien"),
         reverse_mortgage = recode_factor(reverse_mortgage, '1' = "reverse mortgage", 
                                          '2' = "non-reverse mortgage", 
                                          .default = NA_character_),
         open_end_line_of_credit = recode_factor(open_end_line_of_credit, '1' = "open-end LOC", 
                                                 '2' = "not open-end LOC", .default = NA_character_),
         business_or_commercial_purpose = recode_factor(business_or_commercial_purpose,
                                                        '1' = "business purpose", 
                                                        '2' = "non-business purpose", 
                                                        .default = NA_character_),
         applicant_race_1 = recode_factor(applicant_race_1, 
                                          '1' = "Indigenous", 
                                          '2' = "Asian", 
                                          '21' = "Asian",
                                          '3' = "Black",
                                          '5' = "White",
                                          .default = NA_character_),
         applicant_sex = recode_factor(applicant_sex, '1' = "male",
                                       '2' = "female",
                                       '6' = "both", 
                                       .default = NA_character_),
         submission_of_application = recode_factor(submission_of_application, 
                                                   '1' = "submitted",
                                                   '2' = "not submitted", 
                                                   .default = NA_character_),
         initially_payable_to_institution = recode_factor(initially_payable_to_institution, 
                                                          '1' = "payable",
                                                          '2' = "not initially payable", 
                                                          .default = NA_character_),
         occupancy_type = recode_factor(occupancy_type, 
                                        '1' = "principal residence", 
                                        '2' = "second residence",
                                        '3' = "investment property"),
         applicant_age = factor(applicant_age, levels = c("<25", "25-34", "35-44",
                                                          "45-54", "55-64", "65-74", ">74"))) %>%
  mutate(coapplicant_exists = as.factor(ifelse(co_applicant_age != '9999',1,0))) %>%
  #this denotes that a coapplicant exists on the application
  select(-starts_with("co_applicant_"))

```

Since the variable "debt_to_income_ratio" incorporates ranges and whole numbers, I recoded the whole numbers into ranges.

```{r dti_structure}
unique(mortgage_revised$debt_to_income_ratio)
```

```{r recode_dti}
mortgage_revised <- mortgage_revised %>%
  mutate(debt_to_income_ratio = case_when(debt_to_income_ratio == "36" ~ "36%-40%",
                                          debt_to_income_ratio == "37" ~ "36%-40%",
                                          debt_to_income_ratio == "38" ~ "36%-40%",
                                          debt_to_income_ratio == "39" ~ "36%-40%",
                                          debt_to_income_ratio == "40" ~ "36%-40%",
                                          debt_to_income_ratio == "41" ~ "41-45%",
                                          debt_to_income_ratio == "42" ~ "41-45%",
                                          debt_to_income_ratio == "43" ~ "41-45%",
                                          debt_to_income_ratio == "44" ~ "41-45%",
                                          debt_to_income_ratio == "45" ~ "41-45%",
                                          debt_to_income_ratio == "46" ~ "46-49%",
                                          debt_to_income_ratio == "47" ~ "46-49%",
                                          debt_to_income_ratio == "48" ~ "46-49%",
                                          debt_to_income_ratio == "49" ~ "46-49%",
                                          debt_to_income_ratio == "20%-<30%" ~ "20%-<30%",
                                          debt_to_income_ratio == "<20%" ~ "<20%",
                                          debt_to_income_ratio == ">60%" ~ ">60%",
                                          debt_to_income_ratio == "50%-60%" ~ "50%-60%",
                                          debt_to_income_ratio == "30%-<36%" ~ "30%-<36%"))

mortgage_revised <- mortgage_revised %>%
  mutate(debt_to_income_ratio = factor(debt_to_income_ratio,
                                       levels = c("<20%", "20%-<30%", "30%-<36%", "36%-40%",
                                                  "41-45%", "46-49%", "50%-60%", ">60%")))
```

As some final pre-processing steps, I subsequently removed any negative income applicants in my data, because I wanted to log-transform income in my recipe and values that are equal or less than 0 cannot be computed.  This only removed 15 rows, which I will treat as outliers.  All these pre-processing and filtering steps brought my data set size down to 8,593 data points.

```{r filter_out_negatives}
#remove 15 rows
nrow(filter(mortgage_revised, income <= 0))

#updated the main data set to filter out any income below $0
mortgage_revised <- mortgage_revised %>%
  filter(income > 0)
```
## Exploratory Data Analysis

To better understand the data set in question before any models are run, we will explore some of the main patterns and properties of this data set.  First, it is extremely imbalanced, where 97% of the data points are approved mortgages rather than rejected applications.  This makes sense as people are unlikely to submit a mortgage application if they weren't fairly confident that they would get the loan approved. After all, mortgage applications are a lengthy and involved process.

```{r dataimbalance}
knitr::kable(table(mortgage_revised$action_taken))
```
This sample set is also imbalanced on other classes such as race, where the majority of applicants (regardless of their approval status) are White.  While this final project's exercise will focus purely on prediction accuracy, the implications of considering protected traits (or proxies that can derive protected traits) are important to keep in mind as social scientists and to consider balancing our data set on more than just the outcome variable if we were to apply this work in a real-world application.

```{r}
CrossTable(mortgage_revised$action_taken, mortgage_revised$applicant_race_1)
```

Due to the class imbalance on my outcome variable, I used oversampling methods on my recipe steps (using the themis package) to rebalance my data set and improve the balanced accuracy performance of my models.

To lightly investigate qualities that increase the likelihood of mortgage approval, I plotted the natural log of the loan amount and the applicant's income by debt_to_income ratio.  We can see that income and loan amount don't influence mortgage approval decisions as much as metrics like debt-to-income.  In the facets where the ratio was greater than 50%, we can see that few applicants are approved.

```{r ggplot, error = TRUE}
mortgage_revised %>%
  ggplot() +
  geom_point(aes(x = log(income), y = log(loan_amount), color = action_taken)) +
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = "bottom") +
  facet_grid(~debt_to_income_ratio)
```

## Modeling:

Create my training and test sets
```{r splitdata}
set.seed(12345)

mortgage_split <- initial_split(mortgage_revised, strata = state_code, prob = 0.8)
mortgage_train <- training(mortgage_split)
mortgage_test <- testing(mortgage_split)

rm(mortgage, columns_to_remove)
```

I have set up my recipe to consider some interaction variables, impute missing values using the mode if the variable is nominal and using the mean if it is numeric. I also make sure to scale my data.  I did not end up using partial-least squares or PCA in my recipe, though I had strongly considered it given the 50 predictors, because I had issues running my model when either of those steps were incorporated.

```{r recipe, cache = TRUE}

rm(mortgage_split, mortgage_revised )

mortgage_recipe <- recipe(action_taken ~ ., data = mortgage_train) %>%
  step_rm(lei) %>% #remove the unique ID
  step_zv(all_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.8) %>%
  step_log(income, loan_amount) %>% #log transform income and loan_amount
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>% #impute mean for all my numeric predictors
  step_interact(~ (. -action_taken)) %>% #create interaction terms for everything
  step_normalize(all_numeric_predictors()) %>% #scale my numeric data
  step_upsample(action_taken) %>%
  prep(training = mortgage_train)
```

I decided to use cross validation instead of bagging in my GridSearch, because I consider the incorporation of a validation set to evaluate model performance rather than using a pure training set to enhance my test performance.  In this case, my data is split into 10 groups of equal-sized folds where one is left out for validation purposes. While cross-validation typically does not repeat, I chose to repeat the process 2 times.

```{r crossvalidation}
mortgage_folds <- 
   vfold_cv(mortgage_train, strata = state_code, repeats = 2)
```

I ran five models with various tuning parameters to evaluate which model type would yield the best balanced accuracy (a measure that takes an average of specificity and sensitivity metrics).  I chose this metric rather than general accuracy to evaluate my models because my data set is very imbalanced.  The models I ran and their balanced accuracy results are as follows, ranked in descending order:

* MARS (best balanced-accuracy: 0.6900875)
* Random Forest (best balanced-accuracy: 0.6821441)
* NNet Neural Network (best balanced-accuracy: 0.6741368)
* Boosted trees (best balanced-accuracy: 0.6735073)
* Logistic Regression (best balanced-accuracy: 0.6518305)

I have removed logistic regression code from this file, and instead, have showcased the top four models' code, parameters, and results.

**Random Forest Model**
```{r random_forest, cache = TRUE}
gc()
set.seed(12)
#Step 1: Set up random forest model 
rf_model <- 
   rand_forest(min_n = 100, trees = 200) %>%  #i had issues running gridsearch on my randomforest
   set_engine("ranger") %>% 
   set_mode("classification")

#Step 2. Set up workflow
rf_workflow <- 
  workflow() %>%
  add_model(rf_model) %>%
  add_recipe(mortgage_recipe)

#Step 4: Fit model
rf_fit <- fit(rf_workflow, data = mortgage_train)

#Step 5: Predict
rf_pred <- predict(rf_fit, new_data = mortgage_test)

#Step 6: Evaluate my prediction
bind_cols(mortgage_test,rf_pred) %>%
  bal_accuracy(truth = action_taken, estimate = .pred_class)
```

```{r delete_variables1}
rm(rf_workflow, rf_fit, rf_model)
```

**Gradient-Boosting Tree Model**

```{r boosted_trees, cache = TRUE}
gc()

#Step 1: set up model
xgboost_model <- 
  boost_tree(tree_depth = tune(), learn_rate = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 

#Step 2: set up workflow
xgboost_workflow <- 
  workflow() %>% 
  add_recipe(mortgage_recipe) %>% 
  add_model(xgboost_model) 

#Step 3: Tune model
xgboost_grid <- grid_regular(tree_depth(), learn_rate(), levels = 5)

results <- xgboost_workflow %>%
  tune_grid(mortgage_folds,
            grid = xgboost_grid,
            metrics = metric_set(bal_accuracy))

best_accuracy <-
  results %>%
  select_best("bal_accuracy")

finalwf <- finalize_workflow(xgboost_workflow, best_accuracy)

#Step 4: Fit
xgboost_fit <- fit(finalwf, mortgage_train)

#Step 5: Predict
xgboost_pred <- predict(xgboost_fit, mortgage_test)

#Step 6: Eval
bind_cols(mortgage_test, xgboost_pred) %>%
  bal_accuracy(truth = action_taken, estimate = .pred_class)
```

```{r showbest_xg}
show_best(results, "bal_accuracy")
```

```{r delete_variables2}
rm(xgboost_workflow, xgboost_fit, xgboost_grid, xgboost_model, best_accuracy)
```

**MARS Model**

```{r mars, cache = TRUE}
gc()
#mars
mars_model <- mars(num_terms = tune()) %>%
  set_engine("earth") %>%
  set_mode("classification")

mars_wf <- workflow() %>%
  add_model(mars_model) %>%
  add_recipe(mortgage_recipe)

mars_grid <- tibble(num_terms= 1:20)

results <- mars_wf %>%
  tune_grid(mortgage_folds,
            grid = mars_grid,
            metrics = metric_set(bal_accuracy))

best_accuracy <-
  results %>%
  select_best("bal_accuracy")

finalwf <- finalize_workflow(mars_wf, best_accuracy)

mars_fit <- fit(finalwf, mortgage_train)

mars_pred <- predict(mars_fit, mortgage_test)

bind_cols(mortgage_test,mars_pred) %>%
  bal_accuracy(truth = action_taken, estimate = .pred_class)
```

We can evaluate the confusion matrix from our best model.
```{r}
bind_cols(mortgage_test, mars_pred) %>%
  conf_mat(truth = action_taken, estimate = .pred_class)
```
We can view the top performing tuned parameters to confrim that 10 terms was the optimal choice.
```{r}
show_best(results, "bal_accuracy")
```

```{r delete_variables5}
rm(mars_wf, mars_fit, mars_model, best_accuracy)
```

**Neural Net Model**
```{r nnet, cache = TRUE}

nnet_model <- 
  mlp(hidden_units = tune()) %>%
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("classification")

nnet_wf <- workflow() %>%
  add_model(nnet_model) %>%
  add_recipe(mortgage_recipe)

#Step 3: Tune model
nnet_grid <- grid_regular(hidden_units(), levels = 5)

results <- nnet_wf %>%
  tune_grid(mortgage_folds,
            grid = nnet_grid,
            metrics = metric_set(bal_accuracy))

best_accuracy <-
  results %>%
  select_best("bal_accuracy")

finalwf <- finalize_workflow(nnet_wf, best_accuracy)

nnet_fit <- fit(finalwf, mortgage_train)

nnet_pred <- predict(nnet_fit, mortgage_test)

bind_cols(mortgage_test, nnet_pred) %>%
  bal_accuracy(truth = action_taken, estimate = .pred_class)
```

```{r delete_variables10}
rm(nnet_wf, nnet_grid, nnet_fit, best_accuracy)
```


## Results:

Overall, my best model was the MARS model, which yielded a balanced accuracy rate of 0.6900875.  The optimal tuned parameter of "num_terms", which represents the number of features to retain in the final model, was 10, after I tuned the parameter using values of 1 to 20.  Of note, the performance of my top five (5) supervised learning models were relatively similar (within a 5% range), but the MARS model consistently performed best as I tweaked tuning parameters.

With 51 variables in my pre-processed data set (plus the many more variables generated in my recipe due to interaction terms), it makes sense that MARS may have performed best since it automatically conducts feature selection, thus identifying the features that are most important in determining an approval decision.  Likewise, since many of my variables are categorical and not ordinal, many predictors are likely better represented with splines.

## Conclusion:

In completing this project, there were several factors that made a significant contribution and improvement to my overall models' performance.  Addressing the severe class imbalance in my data through up-sampling made the largest difference in improving my balanced accuracy rate.  Before re-sampling, all my models demonstrated 0.5 balanced accuracy (which is no better than guessing).  Accordingly, the high 0.6 balanced accuracy scores that my models yielded after re-balancing was a drastic improvement.

In addition, there were several variables that were removed from my data set during the pre-processing steps that likely contained valuable information for mortgage application decision-making (and prediction performance), such as "combined loan-to-value ratio" (which is the only variable that considers the down-payment amount).  However, due to data quality issues, there were insufficient data points in those columns in my sample of 50,000 applications to merit imputation (after all, I would be making some strong assumptions about using the mean or mode when more than 25% of the data points are missing).  As a next step, it may be worth running a separate set of models on a data set where null values are filtered out from variables like combined loan-to-value ratio.

Overall, this analysis found that MARS works best in this classification problem of prediction which mortgage applications will be approved or rejected.
